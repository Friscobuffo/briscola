{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card:\n",
    "    POINT_SPLITTER = 9\n",
    "    ACE = 1\n",
    "    THREE = 3\n",
    "    POINTS = 8\n",
    "    POINTS_VALUE = 6\n",
    "\n",
    "    def __init__(self, number) -> None:\n",
    "        self.number = number\n",
    "    \n",
    "    def getSeed(self) -> int:\n",
    "        return self.number // 10\n",
    "    \n",
    "    def getValue(self) -> int:\n",
    "        return (self.number % 10) + 1\n",
    "    \n",
    "    def getPoints(self) -> int:\n",
    "        match self.getValue():\n",
    "            case self.ACE: return 11\n",
    "            case self.THREE: return 10\n",
    "            case x if x < self.POINTS: return 0\n",
    "            case x: return x - self.POINTS_VALUE\n",
    "    \n",
    "    def getState(self) -> tuple:\n",
    "        match self.getPoints():\n",
    "                case 0 : cardZone = 0 \n",
    "                case x if x < self.POINT_SPLITTER: cardZone = 1\n",
    "                case _: cardZone = 2\n",
    "             \n",
    "        return (self.getSeed(), cardZone)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deck:\n",
    "    DECK_CARDS = 40\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.cards = []\n",
    "        for i in range(0,self.DECK_CARDS):\n",
    "            self.cards.append(Card(i))\n",
    "        \n",
    "        # randomizing the deck\n",
    "        rnd.shuffle(self.cards)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.__init__()\n",
    "    \n",
    "    def draw(self) -> Card:\n",
    "        return self.cards.pop(0)\n",
    "\n",
    "    def cardsLeft(self) -> int:\n",
    "        return len(self.cards)\n",
    "    \n",
    "    def getLastCard(self) -> Card:\n",
    "        return self.cards[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    HAND_MAX_CARD = 3\n",
    "    CARD_NULL_VALUE = (4,3)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.hand = [] # 3 x Cards (seed = 4 and value = 3 mean null value)\n",
    "        self.oppoOverThreshold = 0 #[0, 1]\n",
    "        self.points = 0\n",
    "        self.wins = 0\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.hand = [] \n",
    "        self.oppoOverThreshold = 0\n",
    "        self.points = 0\n",
    "    \n",
    "    def getState(self) -> tuple:\n",
    "        handState = ()\n",
    "        for card in self.hand:\n",
    "            handState = handState + card.getState()\n",
    "\n",
    "        # enter null values for each missing card\n",
    "        missingCards = self.HAND_MAX_CARD - len(self.hand) \n",
    "        for _ in range(missingCards):\n",
    "            handState = handState + self.CARD_NULL_VALUE\n",
    "\n",
    "        return (self.oppoOverThreshold,) + handState\n",
    "    \n",
    "    def getCard(self, index) -> Card:\n",
    "        return self.hand[index]\n",
    "    \n",
    "    def addCard(self, card) -> None:\n",
    "        self.hand.append(card)\n",
    "    \n",
    "    def removeCard(self, index) -> Card:\n",
    "        return self.hand.pop(index)\n",
    "\n",
    "    def toggleOppoOverThreshold(self) -> None:\n",
    "        self.oppoOverThreshold = 1\n",
    "    \n",
    "    def victoryPassed(self) -> bool:\n",
    "        return self.points > 60\n",
    "    \n",
    "    def getPoints(self) -> int:\n",
    "        return self.points\n",
    "    \n",
    "    def addPoints(self, points) -> None:\n",
    "        self.points += points\n",
    "    \n",
    "    def remainingCards(self) -> int:\n",
    "        return len(self.hand)\n",
    "    \n",
    "    def addWin(self) -> None:\n",
    "        self.wins += 1\n",
    "    \n",
    "    def resetWins(self) -> int:\n",
    "        tmp = self.wins\n",
    "        self.wins = 0\n",
    "        return tmp\n",
    "    \n",
    "    def handIsEmpty(self) -> bool:\n",
    "        return len(self.hand) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    WIN_REWARD = 500\n",
    "    VICTORY_THRESHOLD = 45\n",
    "    BRISCOLA_THRESHOLD = 10\n",
    "    BRISCOLE_THRESHOLD = 7\n",
    "    \n",
    "    Q_STATUS_DIM = (2, 4, 2) + (2, 2, 2, 2) + (2, 5, 4, 5, 4, 5, 4) + (5, 4)\n",
    "    ACTION_DIM = (3,)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.briscolaOverThreshold = 0  # [0, 1]\n",
    "        self.briscolaSeed = None  # [0, 1, 2, 3]\n",
    "        self.briscoleOut = 0\n",
    "        self.briscoleOverThreshold = 0 # [0, 1]\n",
    "        self.loadBySeed = [0, 0, 0, 0] # (denara, spade, bastoni, coppe) [0, 1]\n",
    "        \n",
    "        self.deck = Deck()\n",
    "        self.players = (Player(), Player())\n",
    "    \n",
    "    def getShape(self) -> tuple:\n",
    "        return self.Q_STATUS_DIM + self.ACTION_DIM\n",
    "            \n",
    "    def envState(self) -> tuple:\n",
    "        return (self.briscolaOverThreshold, self.briscolaSeed, self.briscoleOverThreshold) + tuple(self.loadBySeed)\n",
    "    \n",
    "    def getState(self, index) -> tuple:\n",
    "        return self.envState() + self.players[index].getState()\n",
    "    \n",
    "    def getActionState(self, action, index):\n",
    "        return self.players[index].getCard(action).getState()\n",
    "    \n",
    "    def reset(self) -> tuple:\n",
    "        for player in self.players:\n",
    "            player.reset()\n",
    "        self.deck.reset()\n",
    "\n",
    "        # resetting game state info\n",
    "        self.loadBySeed = [0, 0, 0, 0]\n",
    "        self.briscoleOverThreshold = 0\n",
    "\n",
    "        # updating new briscola info\n",
    "        self.briscolaSeed = self.deck.getLastCard().getSeed()\n",
    "        self.briscolaOverThreshold = int(self.deck.getLastCard().getPoints() >= self.BRISCOLA_THRESHOLD)\n",
    "\n",
    "        # dealing cards to the players\n",
    "        for _ in range(3):\n",
    "            for player in self.players:\n",
    "                player.addCard(self.deck.draw())\n",
    "\n",
    "        # returns the pair (fstPlayer state, sndPlayer state)\n",
    "        return (self.getState(0), self.getState(1)) \n",
    "    \n",
    "    def processPlays(self, fstPlay, sndPlay, fstPlayer, sndPlayer) -> tuple:\n",
    "        totPoints = (fstPlay.getPoints() + sndPlay.getPoints())\n",
    "       \n",
    "        winner = sndPlayer\n",
    "        if fstPlay.getSeed() == sndPlay.getSeed():\n",
    "            if fstPlay.getValue() > sndPlay.getValue(): winner = fstPlayer\n",
    "        elif sndPlay.getSeed() != self.briscolaSeed: winner = fstPlayer\n",
    "        \n",
    "        return (winner, totPoints)\n",
    "    \n",
    "    def stateUpdate(self, plays) -> None:\n",
    "        if self.players[0].getPoints() > self.VICTORY_THRESHOLD: self.players[1].toggleOppoOverThreshold()\n",
    "        if self.players[1].getPoints() > self.VICTORY_THRESHOLD: self.players[0].toggleOppoOverThreshold()\n",
    "\n",
    "        # updating briscola counter\n",
    "        for play in plays:\n",
    "            if play.getSeed() == self.briscolaSeed: \n",
    "                self.briscoleOut += 1\n",
    "            if play.getPoints() >= 10: self.loadBySeed[play.getSeed()] = 1\n",
    "        \n",
    "        if self.briscoleOut > self.briscoleOverThreshold: self.briscoleOverThreshold = 1\n",
    "\n",
    "    def step(self, fstPlayerAction, fstPlayerIndex, sndPlayerAction, sndPlayerIndex) -> tuple:\n",
    "        fstPlay = self.players[fstPlayerIndex].removeCard(fstPlayerAction)\n",
    "        sndPlay = self.players[sndPlayerIndex].removeCard(sndPlayerAction)\n",
    "\n",
    "        # evaluating plays and updating points\n",
    "        stepWinner, points = self.processPlays(fstPlay, sndPlay, fstPlayerIndex, sndPlayerIndex)\n",
    "        self.players[stepWinner].addPoints(points)\n",
    "        #print(stepWinner, fstPlayerIndex, (fstPlay.getValue(), fstPlay.getSeed()), (sndPlay.getValue(), sndPlay.getSeed()), self.briscolaSeed)\n",
    "\n",
    "        # generating rewards\n",
    "        if stepWinner == fstPlayerIndex: rewards = [points, -points]\n",
    "        else: rewards = [-points, points]\n",
    "\n",
    "        # updating state info\n",
    "        self.stateUpdate([fstPlay, sndPlay])\n",
    "\n",
    "        # dealing cards\n",
    "        if self.deck.cardsLeft() != 0:\n",
    "            self.players[0].addCard(self.deck.draw())\n",
    "            self.players[1].addCard(self.deck.draw())\n",
    "\n",
    "        # checking players victory\n",
    "        done = False\n",
    "        \n",
    "        for i in range(len(self.players)):\n",
    "            if self.players[i].victoryPassed():\n",
    "                done = True\n",
    "                self.players[i].addWin()\n",
    "                if i == fstPlayerIndex: \n",
    "                    rewards[0] += self.WIN_REWARD\n",
    "                    rewards[1] -= self.WIN_REWARD\n",
    "                else: \n",
    "                    rewards[0] -= self.WIN_REWARD\n",
    "                    rewards[1] += self.WIN_REWARD\n",
    "                break\n",
    "        \n",
    "        # nobody won if both players has empty hand\n",
    "        if self.players[0].handIsEmpty() and not done:\n",
    "            done = True\n",
    "            \n",
    "        return (self.getState(fstPlayerIndex), self.getState(sndPlayerIndex), rewards[0], rewards[1], \n",
    "                done, self.players[fstPlayerIndex].remainingCards(), self.players[sndPlayerIndex].remainingCards(), stepWinner)\n",
    "    \n",
    "    def getMatchStats(self) -> tuple:\n",
    "        return (self.players[0].resetWins(), self.players[1].resetWins())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IA:\n",
    "    EPS_THRESHOLD = 0.01\n",
    "    TEST_THRESHOLD = 1000000\n",
    "    TEST_EPISODES = 10000\n",
    "    HAND_MAX_CARD = 3\n",
    "    NULL_OPPO_PLAY = (4,3)\n",
    "    FST_PLAYER_ID = 0\n",
    "    SND_PLAYER_ID = 1\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.env = Environment()\n",
    "        self.Q = []\n",
    "        self.trainedQ = []\n",
    "    \n",
    "    def randomAction(self, remainingCards = HAND_MAX_CARD) -> int:\n",
    "        return rnd.randint(0, remainingCards-1)\n",
    "\n",
    "    def epsGreedy(self, state, eps=0.1, remainingCards = HAND_MAX_CARD) -> int:\n",
    "        # Epsilon greedy policy\n",
    "        if np.random.uniform(0,1) < eps:\n",
    "            # Choose a random action\n",
    "            return self.randomAction(remainingCards)\n",
    "        else:\n",
    "            # Choose the action of a greedy policy\n",
    "            return self.greedy(state)\n",
    "\n",
    "\n",
    "    def greedy(self, state) -> int:\n",
    "        #Greedy policy\n",
    "        #return the index corresponding to the maximum action-state value\n",
    "        return np.argmax(self.Q[state])\n",
    "    \n",
    "    def runTest(self, numEpisodes=100, toPrint=False) -> tuple:\n",
    "        # Run some episodes to test the policy against random player\n",
    "        # in this case Q2 represents random player\n",
    "        rewards = []\n",
    "        fstStarting = True\n",
    "\n",
    "        for _ in range(numEpisodes):\n",
    "            done = False\n",
    "            episodeRew = 0\n",
    "            \n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            # choosing who starts the game\n",
    "            if(fstStarting):\n",
    "                # generating fstPlayer action\n",
    "                state = state + self.NULL_OPPO_PLAY # null value on the opponent play\n",
    "                fstAction = self.greedy(state)\n",
    "                    \n",
    "                # generating sndPlayer action\n",
    "                sndAction = self.randomAction()\n",
    "                \n",
    "                # take one step in the environment\n",
    "                state, _, stepReward, _, done, _, sndCards, stepWinner = self.env.step(fstAction, self.FST_PLAYER_ID, sndAction, self.SND_PLAYER_ID)\n",
    "            else:\n",
    "                # generating sndPlayer action\n",
    "                sndAction = self.randomAction()\n",
    "\n",
    "                #  generating fstPlayer action\n",
    "                state = state + self.env.getActionState(sndAction, self.SND_PLAYER_ID)\n",
    "                fstAction = self.greedy(state)\n",
    "                    \n",
    "                # take one step in the environment\n",
    "                _, state, _, stepReward, done, sndCards, _, stepWinner = self.env.step(sndAction, self.SND_PLAYER_ID, fstAction, self.FST_PLAYER_ID)\n",
    "            \n",
    "            episodeRew += stepReward\n",
    "\n",
    "            #playing the game\n",
    "            while not done:\n",
    "                # if fstPlayer is playing first\n",
    "                if(stepWinner == self.FST_PLAYER_ID):\n",
    "                    # generating fstPLayer action\n",
    "                    state = state + self.NULL_OPPO_PLAY # null value on the opponent play\n",
    "                    fstAction = self.greedy(state)\n",
    "                    \n",
    "                    # generating sndPLayer action\n",
    "                    sndAction = self.randomAction(sndCards)\n",
    "                    \n",
    "                    # take one step in the environment\n",
    "                    state, _, stepReward, _, done, _, sndCards, stepWinner = self.env.step(fstAction, self.FST_PLAYER_ID, sndAction, self.SND_PLAYER_ID)\n",
    "                \n",
    "                else: # if fstPlayer is playing second\n",
    "                    # generating sndPLayer action\n",
    "                    sndAction = self.randomAction(sndCards)\n",
    "\n",
    "                    #  generating fstPLayer action\n",
    "                    state = state + self.env.getActionState(sndAction, self.SND_PLAYER_ID)\n",
    "                    fstAction = self.greedy(state)\n",
    "                    \n",
    "                    # take one step in the environment\n",
    "                    _, state, _, stepReward, done, sndCards, _, stepWinner = self.env.step(sndAction, self.SND_PLAYER_ID, fstAction, self.FST_PLAYER_ID)\n",
    "\n",
    "                episodeRew += stepReward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(episodeRew)\n",
    "                    fstStarting = not fstStarting\n",
    "\n",
    "        matchStats = self.env.getMatchStats()\n",
    "        winPercentage =  (matchStats[0] * 100) / numEpisodes\n",
    "\n",
    "        if toPrint:\n",
    "            print('Mean score: %.3f Win percentage: %.2f out of %i games!'%(np.mean(rewards), winPercentage, numEpisodes))\n",
    "\n",
    "        return (np.mean(rewards), winPercentage)\n",
    "    \n",
    "    # research for optimal policy Q\n",
    "    def sarsaLearning(self, numEpisodes=10000, alpha=0.01, eps=0.3, gamma=0.95, epsDecay=0.00005) -> None:\n",
    "        # briscola need two players\n",
    "        QShape = self.env.getShape()\n",
    "        self.Q = np.zeros(QShape) # training player\n",
    "        self.trainedQ = self.Q.copy() # trained player\n",
    "        \n",
    "        fstStarting = True\n",
    "\n",
    "        for ep in range(numEpisodes):\n",
    "            fstState, sndState = self.env.reset() # initial iniziale for Q1 and Q2\n",
    "            done = False\n",
    "\n",
    "            # choosing who starts the game\n",
    "            if(fstStarting):\n",
    "                # generating fstPlayer action\n",
    "                fstState = fstState + self.NULL_OPPO_PLAY # null value on the opponent play\n",
    "                fstAction = self.epsGreedy(fstState, eps)\n",
    "                    \n",
    "                # generating sndPlayer action\n",
    "                sndState = sndState + self.env.getActionState(fstAction, self.FST_PLAYER_ID)\n",
    "                sndAction = self.greedy(sndState)\n",
    "                \n",
    "                # take one step in the environment\n",
    "                fstNextState, sndNextState, stepReward, _, done, fstCards, _, stepWinner = self.env.step(fstAction, self.FST_PLAYER_ID, sndAction, self.SND_PLAYER_ID)\n",
    "            else:\n",
    "                # generating sndPlayer action\n",
    "                sndState = sndState + self.NULL_OPPO_PLAY\n",
    "                sndAction = self.greedy(sndState)\n",
    "\n",
    "                #  generating fstPLayer action\n",
    "                fstState = fstState + self.env.getActionState(sndAction, self.SND_PLAYER_ID)\n",
    "                fstAction = self.epsGreedy(fstState, eps)\n",
    "                    \n",
    "                # take one step in the environment\n",
    "                sndNextState, fstNextState, _, stepReward, done, _, fstCards, stepWinner = self.env.step(sndAction, self.SND_PLAYER_ID, fstAction, self.FST_PLAYER_ID)\n",
    "\n",
    "            # loop the main body until the environment stops\n",
    "            while not done:\n",
    "                # checking who's next playing first  \n",
    "                # if first player won this round must play first            \n",
    "                if(stepWinner == self.FST_PLAYER_ID): \n",
    "                    # generating first player next action (needed for the SARSA update)\n",
    "                    fstNextState = fstNextState + self.NULL_OPPO_PLAY\n",
    "                    fstNextAction = self.epsGreedy(fstNextState, eps, fstCards)\n",
    "                \n",
    "                    # generating second player next action\n",
    "                    sndNextState = sndNextState + self.env.getActionState(fstNextAction, self.FST_PLAYER_ID)\n",
    "                    sndNextAction = self.greedy(sndNextState)\n",
    "                \n",
    "                else: # if first player lost must play second \n",
    "                    # generating second player next action\n",
    "                    sndNextState = sndNextState + self.NULL_OPPO_PLAY\n",
    "                    sndNextAction = self.greedy(sndNextState)\n",
    "\n",
    "                    # generating first player next action\n",
    "                    fstNextState = fstNextState + self.env.getActionState(sndNextAction, self.SND_PLAYER_ID)\n",
    "                    fstNextAction = self.epsGreedy(fstNextState, eps, fstCards)\n",
    "\n",
    "                # SARSA update\n",
    "                self.Q[fstState][fstAction] = self.Q[fstState][fstAction] \n",
    "                + alpha * (stepReward + gamma * self.Q[fstNextState][fstNextAction] - self.Q[fstState][fstAction])\n",
    "\n",
    "                # updating players states and actions\n",
    "                fstState, sndState = fstNextState, sndNextState\n",
    "                fstAction, sndAction = fstNextAction, sndNextAction\n",
    "\n",
    "                # take one step in the environment\n",
    "                if(stepWinner == self.FST_PLAYER_ID):       \n",
    "                    fstNextState, sndNextState, stepReward, _, done, fstCards, _, stepWinner = self.env.step(fstAction, self.FST_PLAYER_ID, sndAction, self.SND_PLAYER_ID)\n",
    "                else:\n",
    "                    sndNextState, fstNextState, _, stepReward, done, _, fstCards, stepWinner = self.env.step(sndAction, self.SND_PLAYER_ID, fstAction, self.FST_PLAYER_ID)\n",
    "            \n",
    "            # every few episodes trainedQ is updated\n",
    "            if (ep % self.TEST_THRESHOLD) == 0:\n",
    "                # updating trainedQ\n",
    "                self.trainedQ = self.Q.copy()\n",
    "\n",
    "                # decay the epsilon value until it reaches the threshold\n",
    "                if eps > self.EPS_THRESHOLD: eps -= epsDecay\n",
    "\n",
    "                # training stats\n",
    "                trainingStats = self.env.getMatchStats()\n",
    "\n",
    "                # testing policy Q against random player\n",
    "                avgRew, winPercentage = self.runTest(self.TEST_EPISODES)\n",
    "                print('Episode:%.5d  Epsylon: %.4f  Average Reward: %.4f Win Percentage: %.2f' %(ep, eps, avgRew, winPercentage))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:00000  Epsylon: 0.2999  Average Reward: -61.3921 Win Percentage: 43.35\n",
      "Episode:1000000  Epsylon: 0.2999  Average Reward: -44.7899 Win Percentage: 44.83\n",
      "Episode:2000000  Epsylon: 0.2999  Average Reward: -48.8130 Win Percentage: 44.45\n",
      "Episode:3000000  Epsylon: 0.2998  Average Reward: -46.5522 Win Percentage: 44.70\n",
      "Episode:4000000  Epsylon: 0.2998  Average Reward: -58.1482 Win Percentage: 43.55\n",
      "Episode:5000000  Epsylon: 0.2997  Average Reward: -53.8061 Win Percentage: 43.97\n",
      "Episode:6000000  Epsylon: 0.2997  Average Reward: -52.5580 Win Percentage: 44.10\n",
      "Episode:7000000  Epsylon: 0.2996  Average Reward: -58.7352 Win Percentage: 43.57\n",
      "Episode:8000000  Epsylon: 0.2996  Average Reward: -56.2212 Win Percentage: 43.82\n",
      "Episode:9000000  Epsylon: 0.2995  Average Reward: -47.4702 Win Percentage: 44.68\n",
      "Episode:10000000  Epsylon: 0.2995  Average Reward: -52.0588 Win Percentage: 44.14\n",
      "Episode:11000000  Epsylon: 0.2994  Average Reward: -52.4846 Win Percentage: 44.08\n",
      "Episode:12000000  Epsylon: 0.2994  Average Reward: -53.9667 Win Percentage: 43.97\n",
      "Episode:13000000  Epsylon: 0.2993  Average Reward: -56.3451 Win Percentage: 43.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ia \u001b[39m=\u001b[39m IA()\n\u001b[0;32m----> 2\u001b[0m ia\u001b[39m.\u001b[39;49msarsaLearning(\u001b[39m100000000\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[62], line 156\u001b[0m, in \u001b[0;36mIA.sarsaLearning\u001b[0;34m(self, numEpisodes, alpha, eps, gamma, epsDecay)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39m# generating second player next action\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     sndNextState \u001b[39m=\u001b[39m sndNextState \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mgetActionState(fstNextAction, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFST_PLAYER_ID)\n\u001b[0;32m--> 156\u001b[0m     sndNextAction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy(sndNextState)\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m: \u001b[39m# if first player lost must play second \u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[39m# generating second player next action\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     sndNextState \u001b[39m=\u001b[39m sndNextState \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNULL_OPPO_PLAY\n",
      "Cell \u001b[0;32mIn[62], line 31\u001b[0m, in \u001b[0;36mIA.greedy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgreedy\u001b[39m(\u001b[39mself\u001b[39m, state) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m     29\u001b[0m     \u001b[39m#Greedy policy\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39m#return the index corresponding to the maximum action-state value\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49margmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mQ[state])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/briscola/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/briscola/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ia = IA()\n",
    "ia.sarsaLearning(100000000, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "briscola",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
